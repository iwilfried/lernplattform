# ðŸŽ² Markov Decision Process

Die mathematischen Grundlagen des Reinforcement Learning verstehen und anwenden.

## ðŸ§® MDP Komponenten

Ein MDP ist definiert durch das Tupel (S, A, P, R, Î³):
- **S** - Zustandsraum (States)
- **A** - Aktionsraum (Actions) 
- **P** - Ãœbergangswahrscheinlichkeiten
- **R** - Belohnungsfunktion
- **Î³** - Diskontierungsfaktor

## ðŸ“Š Bellman-Gleichung

```python
def bellman_equation(state, action, transition_probs, rewards, gamma=0.9):
    """
    V(s) = max_a Î£ P(s'|s,a) [R(s,a,s') + Î³V(s')]
    """
    expected_return = 0
    for next_state, prob in transition_probs[state][action].items():
        reward = rewards[state][action][next_state]
        expected_return += prob * (reward + gamma * value_function[next_state])
    
    return expected_return
```

## ðŸŽ¯ Value Iteration

```python
def value_iteration(mdp, threshold=0.01):
    V = {s: 0 for s in mdp.states}
    
    while True:
        delta = 0
        for state in mdp.states:
            v = V[state]
            V[state] = max([bellman_equation(state, a, mdp) for a in mdp.actions])
            delta = max(delta, abs(v - V[state]))
        
        if delta < threshold:
            break
    
    return V
```

---

**â±ï¸ GeschÃ¤tzte Zeit**: 40 Minuten  
**ðŸŽ¯ Schwierigkeit**: Advanced 